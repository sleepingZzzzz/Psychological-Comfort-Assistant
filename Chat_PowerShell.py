import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer
from peft import PeftModel
import os
# --- 1. é…ç½®æ¨¡å‹è·¯å¾„ ---
# ğŸ”´ è¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µä¿®æ”¹è¿™é‡Œçš„è·¯å¾„

# åŸºç¡€æ¨¡å‹è·¯å¾„ (å¯ä»¥æ˜¯ Hugging Face Hub çš„ IDï¼Œä¹Ÿå¯ä»¥æ˜¯æ‚¨æœ¬åœ°ä¸‹è½½çš„è·¯å¾„)
# å¦‚æœæ‚¨ä¹‹å‰å·²ç»å°†åŸºç¡€æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨æœ¬åœ°è·¯å¾„ä»¥åŠ å¿«åŠ è½½é€Ÿåº¦
base_model_path = "Qwen/Qwen1.5-7B-Chat"  # å‡è®¾æ‚¨ä¹‹å‰æ¢æˆäº† 4B æ¨¡å‹

# æ‚¨è®­ç»ƒå¥½çš„ LoRA/DoRA é€‚é…å™¨æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„
# è¿™åº”è¯¥æ˜¯æ‚¨ä¸Šä¸€è½®è®­ç»ƒæ—¶ 'output_dir' æŒ‡å®šçš„é‚£ä¸ªæ–‡ä»¶å¤¹
adapter_path = "./mindchat-qwen1.5-7B-finetuned" 

# --- 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---

print("... æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ ...")

# ä½¿ç”¨ 4-bit é‡åŒ–åŠ è½½åŸºç¡€æ¨¡å‹ï¼ŒèŠ‚çœæ˜¾å­˜
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

print("... æ­£åœ¨åŠ è½½å¹¶èåˆ LoRA/DoRA é€‚é…å™¨ ...")

# åŠ è½½ PEFT é€‚é…å™¨å¹¶å°†å…¶ä¸åŸºç¡€æ¨¡å‹èåˆ
# è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼ŒPeftModel ä¼šè‡ªåŠ¨å®Œæˆâ€œç©¿ä¸Šç¥è£…â€çš„æ“ä½œ
model = PeftModel.from_pretrained(base_model, adapter_path)

# å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()

print("âœ… æ¨¡å‹å‡†å¤‡å°±ç»ªï¼å¼€å§‹å¯¹è¯å§ (è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º)ã€‚\n")

# --- 3. å¼€å§‹äº¤äº’å¼å¯¹è¯ ---
streamer = TextStreamer(tokenizer, skip_prompt=True)
# å­˜å‚¨å¯¹è¯å†å²
history = []

while True:
    try:
        user_input = input("You: ")
        # ...
        history.append({"role": "user", "content": user_input})
        messages = tokenizer.apply_chat_template(
            history, 
            tokenize=False, 
            add_generation_prompt=True
        )
        model_inputs = tokenizer([messages], return_tensors="pt").to("cuda")

        # ğŸ”´ 3. åœ¨ generate å‡½æ•°ä¸­ï¼Œä¼ å…¥ streamer å‚æ•°
        #    æ³¨æ„ï¼šæˆ‘ä»¬ä¸å†éœ€è¦è‡ªå·±è§£ç ï¼Œstreamer ä¼šè‡ªåŠ¨å¤„ç†æ‰“å°
        print("Assistant: ", end="") # å…ˆæ‰“å°å‡º "Assistant: "
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            repetition_penalty=1.1,
            streamer=streamer # <--- æ ¸å¿ƒæ”¹åŠ¨åœ¨è¿™é‡Œ
        )

        # ğŸ”´ 4. å› ä¸º streamer å·²ç»å®Œæˆäº†æ‰“å°ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è·å–å®Œæ•´å›ç­”å¹¶å­˜å…¥å†å²
        #    è¿™ä¸€æ­¥ç¨å¾®å¤æ‚ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è§£ç å®Œæ•´çš„è¾“å‡º
        full_response_ids = generated_ids[0][model_inputs.input_ids.shape[-1]:]
        response = tokenizer.decode(full_response_ids, skip_special_tokens=True)
        history.append({"role": "assistant", "content": response})

    except KeyboardInterrupt:
        break

print("\nğŸ‘‹ å¯¹è¯ç»“æŸã€‚")