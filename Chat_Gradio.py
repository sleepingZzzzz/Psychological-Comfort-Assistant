import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextIteratorStreamer
from peft import PeftModel
import gradio as gr
from threading import Thread

# --- å…¨å±€å˜é‡ ---
model = None
tokenizer = None

# --- 1. æ¨¡å‹åŠ è½½å‡½æ•° (æœ€ç»ˆä¿®æ­£ç‰ˆ) ---
def load_model(base_model_path, adapter_path):
    global model, tokenizer
    if not base_model_path:
        return "é”™è¯¯ï¼šåŸºç¡€æ¨¡å‹è·¯å¾„ä¸èƒ½ä¸ºç©ºã€‚"

    print(f"... æ­£åœ¨ä»æœ¬åœ°è·¯å¾„ '{base_model_path}' åŠ è½½åŸºç¡€æ¨¡å‹ ...")
    
    try:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )

        # ğŸ”´ å…³é”®ç‚¹ 1: ä¸ºåŠ è½½åŸºç¡€æ¨¡å‹å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True  # <--- ç¡®ä¿è¿™é‡Œæœ‰
        )

        # ğŸ”´ å…³é”®ç‚¹ 2: ä¸ºåŠ è½½åˆ†è¯å™¨å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            local_files_only=True  # <--- ç¡®ä¿è¿™é‡Œæœ‰
        )
        tokenizer.padding_side = 'left'

        # å¦‚æœç”¨æˆ·æä¾›äº†é€‚é…å™¨è·¯å¾„ï¼Œå°±åŠ è½½å®ƒ
        if adapter_path:
            print(f"... æ­£åœ¨ä»æœ¬åœ°è·¯å¾„ '{adapter_path}' åŠ è½½å¹¶èåˆ LoRA/DoRA é€‚é…å™¨ ...")
            # ğŸ”´ å…³é”®ç‚¹ 3: ä¸ºåŠ è½½é€‚é…å™¨ä¹Ÿå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            model = PeftModel.from_pretrained(
                base_model, 
                adapter_path,
                local_files_only=True # <--- ç¡®ä¿è¿™é‡Œä¹Ÿæœ‰
            )
        else:
            print("... æœªæä¾›é€‚é…å™¨è·¯å¾„ï¼Œå°†ç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹ ...")
            model = base_model
        
        model.eval()

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return "æ¨¡å‹åŠ è½½æˆåŠŸï¼æ‚¨ç°åœ¨å¯ä»¥å¼€å§‹å¯¹è¯äº†ã€‚"
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

# --- 2. æ ¸å¿ƒå¯¹è¯/ç”Ÿæˆå‡½æ•° (æ— éœ€æ”¹åŠ¨) ---
def chat(message, history):
    if not history:
            history.append({
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä½“è´´ã€æ¸©æŸ”çš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹ã€‚ä½ çš„æ‰€æœ‰å›ç­”éƒ½åº”è¯¥ç®€æ´ä¸”æ¸©æš–, åœ¨æ¯æ®µå›ç­”çš„ç»“å°¾éƒ½åŠ ä¸Šä¸€æ®µé¼“åŠ±çš„è¯å¹¶åŠ ä¸Šå‡ ä¸ªè¡¨æƒ…, ä¸¥æ ¼é™åˆ¶æ¯æ¬¡å›ç­”çš„å­—æ•°ä¸è¶…è¿‡200ä¸ªå­—ã€‚"
            })
            
    history.append({"role": "user", "content": message})
    if model is None or tokenizer is None:
        history.append({"role": "assistant", "content": "é”™è¯¯ï¼šè¯·å…ˆç‚¹å‡»â€œåŠ è½½æ¨¡å‹â€æŒ‰é’®ï¼Œå¹¶ç­‰å¾…åŠ è½½æˆåŠŸã€‚"})
        yield history
        return

    conversation = history
    messages = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([messages], return_tensors="pt").to("cuda")
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    generation_kwargs = dict(**model_inputs, streamer=streamer, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.95, repetition_penalty=1.1, eos_token_id=tokenizer.eos_token_id)
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    history.append({"role": "assistant", "content": ""})
    for new_text in streamer:
            # åœ¨å°†æ–‡æœ¬æ·»åŠ åˆ°å†å²ä¹‹å‰ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«äº†ç»“æŸæ ‡è®°
            if tokenizer.eos_token in new_text:
                # å¦‚æœåŒ…å«ï¼Œå¯èƒ½æ„å‘³ç€ä¸€ä¸ªä¸å®Œæ•´çš„æ ‡è®°ï¼Œæˆ‘ä»¬æœ€å¥½åœ¨è¿™é‡Œç›´æ¥åœæ­¢
                break
            history[-1]["content"] += new_text
            yield history

# --- 3. Gradio ç•Œé¢å¸ƒå±€ (æ— éœ€æ”¹åŠ¨) ---
with gr.Blocks(theme=gr.themes.Soft()) as app:
    gr.Markdown("# ğŸ§  MindChat å¾®è°ƒæ¨¡å‹æµ‹è¯•ç•Œé¢")
    with gr.Row():
        with gr.Column(scale=1):
            base_model_input = gr.Textbox(label="åŸºç¡€æ¨¡å‹æœ¬åœ°è·¯å¾„", value="./PreModel/models--Qwen--Qwen1.5-7B-Chat/snapshots/5f4f5e69ac7f1d508f8369e977de208b4803444b")
            adapter_path_input = gr.Textbox(label="é€‚é…å™¨ (Adapter) æœ¬åœ°è·¯å¾„", value="./mindchat-qwen1.5-7b-finetuned")
            load_button = gr.Button("åŠ è½½æ¨¡å‹", variant="primary")
            status_text = gr.Textbox(label="çŠ¶æ€", interactive=False)
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(label="å¯¹è¯çª—å£", height=500, type="messages")
            msg_input = gr.Textbox(label="æ‚¨çš„é—®é¢˜", placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œç„¶åæŒ‰ Enter...")
            clear_button = gr.Button("æ¸…é™¤å¯¹è¯å†å²")
    load_button.click(fn=load_model, inputs=[base_model_input, adapter_path_input], outputs=[status_text])
    msg_input.submit(fn=chat, inputs=[msg_input, chatbot], outputs=[chatbot])
    msg_input.submit(lambda: "", None, msg_input)
    clear_button.click(lambda: None, None, chatbot, queue=False)

# --- 5. å¯åŠ¨åº”ç”¨ ---
if __name__ == "__main__":
    app.launch()