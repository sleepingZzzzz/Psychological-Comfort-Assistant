import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer
import os

# --- 1. é…ç½®æ¨¡å‹è·¯å¾„ ---
base_model_path = "Qwen/Qwen1.5-7B-Chat" 

# --- 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---

print("... æ­£åœ¨åŠ è½½åŸå§‹åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ ...")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)

# ğŸ”´ å…³é”®æ”¹åŠ¨ 1: ä¸å†å°† pad_token è®¾ç½®ä¸º eos_tokenï¼Œä»¥æ¶ˆé™¤æ­§ä¹‰
# tokenizer.pad_token = tokenizer.eos_token  <--- åˆ é™¤æˆ–æ³¨é‡Šæ‰æ­¤è¡Œ

# ğŸ”´ å…³é”®æ”¹åŠ¨ 2: å¯¹äºç”Ÿæˆä»»åŠ¡ï¼Œæ˜ç¡®æŒ‡å®šå¡«å……åœ¨å·¦ä¾§ã€‚è¿™æ˜¯ decoder-only æ¨¡å‹çš„æœ€ä½³å®è·µ
tokenizer.padding_side = 'left'

model.eval()

print("âœ… åŸå§‹æ¨¡å‹å‡†å¤‡å°±ç»ªï¼å¼€å§‹å¯¹è¯å§ (è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º)ã€‚\n")

# --- 3. å¼€å§‹äº¤äº’å¼å¯¹è¯ ---

streamer = TextStreamer(tokenizer, skip_prompt=True)
history = []

while True:
    try:
        user_input = input("You (Base Model): ")
        
        if user_input.lower() in ["exit", "quit"]:
            break

        history.append({"role": "user", "content": user_input})
        
        messages = tokenizer.apply_chat_template(
            history, 
            tokenize=False, 
            add_generation_prompt=True
        )

        model_inputs = tokenizer([messages], return_tensors="pt").to("cuda")

        print("Assistant (Base Model): ", end="")
        generated_ids = model.generate(
            **model_inputs,
            # ğŸ”´ å…³é”®æ”¹åŠ¨ 3: æ˜ç¡®å‘Šè¯‰ generate å‡½æ•°ï¼Œå“ªä¸ªæ˜¯ç»“æŸæ ‡è®°
            eos_token_id=tokenizer.eos_token_id,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            repetition_penalty=1.1,
            streamer=streamer
        )

        full_response_ids = generated_ids[0][model_inputs.input_ids.shape[-1]:]
        response = tokenizer.decode(full_response_ids, skip_special_tokens=True)
        history.append({"role": "assistant", "content": response})

    except KeyboardInterrupt:
        break

print("\nğŸ‘‹ å¯¹è¯ç»“æŸã€‚")