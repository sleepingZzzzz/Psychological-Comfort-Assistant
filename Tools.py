from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import argparse

# ğŸ”´ å·²å°† load_model_and_tokenizer å‡½æ•°è¿˜åŸ
def load_model_and_tokenizer(model_name, quantization_config):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ (ä»Hugging Face Hub), åº”ç”¨ 4-bit é‡åŒ–ã€‚"""
    print("... æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ...")
    
    bnb_config = BitsAndBytesConfig(**quantization_config)

    # local_files_only=True å·²è¢«ç§»é™¤
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    # local_files_only=True å·²è¢«ç§»é™¤
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    
    print("âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å®Œæˆ")
    return model, tokenizer

def prepare_dataset(dataset_name, tokenizer):
    """
    åŠ è½½å¹¶é¢„å¤„ç†æ–°çš„å¯¹è¯æ•°æ®é›†ã€‚
    è¯¥å‡½æ•°ç›´æ¥åˆ©ç”¨ 'extra_info' åˆ—ä¸­çš„ 'full_conversation' æ•°æ®ã€‚
    """
    print("... æ­£åœ¨å‡†å¤‡æ–°çš„å¯¹è¯æ•°æ®é›† ...")
    
    # å‡è®¾æ•°æ®é›†åªæœ‰ä¸€ä¸ª 'train' åˆ†å‰²
    dataset = load_dataset(dataset_name, split="train")

    def preprocess_function(examples):
        """
        é¢„å¤„ç†å‡½æ•°ï¼šç›´æ¥åº”ç”¨èŠå¤©æ¨¡æ¿åˆ° 'full_conversation' åˆ—ã€‚
        """
        # 1. ä» 'extra_info' åˆ—ä¸­æå– 'full_conversation' åˆ—è¡¨
        # 'examples['extra_info']' æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå­—å…¸
        all_conversations = [info['full_conversation'] for info in examples['extra_info']]
        
        # 2. å¯¹æ¯ä¸ªå¯¹è¯åˆ—è¡¨åº”ç”¨èŠå¤©æ¨¡æ¿
        # å› ä¸ºæ•°æ®æ ¼å¼å·²ç»å®Œç¾åŒ¹é…ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å°†å…¶ä¼ é€’ç»™ apply_chat_template
        processed_conversations = [
            tokenizer.apply_chat_template(
                conv, 
                tokenize=False, 
                add_generation_prompt=False
            ) 
            for conv in all_conversations
        ]

        # 3. å¯¹æ ¼å¼åŒ–åçš„æ–‡æœ¬è¿›è¡Œ tokenize
        tokenized_inputs = tokenizer(
            processed_conversations,
            max_length=512, # æ•°å€¼è¶Šå¤§ï¼Œæ˜¾å­˜å ç”¨è¶Šå¤§ï¼Œè®­ç»ƒæ•ˆæœè¶Šå¥½
            truncation=True,
            padding=False,
        )
        return tokenized_inputs

    processed_dataset = dataset.map(
        preprocess_function, 
        batched=True, 
        remove_columns=dataset.column_names
    )
    
    print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ")
    return processed_dataset

def setup_trainer(model, tokenizer, dataset, lora_config_dict, training_config_dict):
    """è®¾ç½® PEFT/LoRA å¹¶é…ç½®è®­ç»ƒå™¨ã€‚"""
    print("... æ­£åœ¨è®¾ç½®è®­ç»ƒå™¨ ...")
    
    model = prepare_model_for_kbit_training(model)
    lora_config = LoraConfig(**lora_config_dict)
    model = get_peft_model(model, lora_config)
    
    model.print_trainable_parameters()

    training_args = TrainingArguments(**training_config_dict)

    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    model.config.use_cache = False
    
    print("âœ… è®­ç»ƒå™¨è®¾ç½®å®Œæˆ")
    return trainer

# --- ğŸ”´ å®šä¹‰å‘½ä»¤è¡Œå‚æ•°è§£æ (å·²æ‰©å±•) ---
def parse_args():
    parser = argparse.ArgumentParser(description="å¾®è°ƒè¯­è¨€æ¨¡å‹çš„è„šæœ¬")
    
    # æ¨¡å‹ä¸æ•°æ®
    parser.add_argument("--model_name", type=str, help="è¦å¾®è°ƒçš„åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--dataset_name", type=str, help="ç”¨äºå¾®è°ƒçš„æ•°æ®é›†åç§°æˆ–è·¯å¾„")
    
    # LoRA / DoRA å‚æ•°
    parser.add_argument("--lora_r", type=int, help="LoRA/DoRA çš„ç§© r")
    parser.add_argument("--lora_alpha", type=int, help="LoRA/DoRA çš„ alpha å€¼")
    parser.add_argument("--lora_dropout", type=float, help="LoRA å±‚çš„ dropout ç‡")
    # ğŸ”´ æ–°å¢ use_dora å¸ƒå°”å¼€å…³
    # action=argparse.BooleanOptionalAction ä¼šè‡ªåŠ¨åˆ›å»º --use_dora å’Œ --no-use_dora ä¸¤ä¸ª flag
    parser.add_argument("--use_dora", action=argparse.BooleanOptionalAction, help="æ˜¯å¦ä½¿ç”¨ DoRA æŠ€æœ¯")

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--output_dir", type=str, help="è®­ç»ƒè¾“å‡ºçš„ç›®å½•")
    parser.add_argument("--num_train_epochs", type=int, help="è®­ç»ƒçš„æ€»è½®æ¬¡")
    parser.add_argument("--learning_rate", type=float, help="å­¦ä¹ ç‡")
    parser.add_argument("--logging_steps", type=int, help="æ¯éš”å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡ log")
    parser.add_argument("--per_device_train_batch_size", type=int, help="æ¯ä¸ªè®¾å¤‡çš„ batch size")
    parser.add_argument("--gradient_accumulation_steps", type=int, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    
    args = parser.parse_args()
    return args

# --- ğŸ”´ æ›´æ–°é»˜è®¤é…ç½®çš„å‡½æ•° (å·²æ‰©å±•) ---
def update_config(config, args):
    # ä½¿ç”¨ vars(args) å°† argparse.Namespace å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ï¼Œæ–¹ä¾¿éå†
    args_dict = vars(args)
    
    # æ£€æŸ¥å¹¶æ›´æ–°æä¾›çš„å‚æ•°
    if args_dict.get("model_name") is not None:
        config['model_name'] = args_dict["model_name"]
    if args_dict.get("dataset_name") is not None:
        config['dataset_name'] = args_dict["dataset_name"]
        
    # æ›´æ–° lora å‚æ•°
    if args_dict.get("lora_r") is not None:
        config['lora']['r'] = args_dict["lora_r"]
    if args_dict.get("lora_alpha") is not None:
        config['lora']['lora_alpha'] = args_dict["lora_alpha"]
    if args_dict.get("lora_dropout") is not None:
        config['lora']['lora_dropout'] = args_dict["lora_dropout"]
    if args_dict.get("use_dora") is not None:
        config['lora']['use_dora'] = args_dict["use_dora"]

    # æ›´æ–° training å‚æ•°
    if args_dict.get("output_dir") is not None:
        config['training']['output_dir'] = args_dict["output_dir"]
    if args_dict.get("num_train_epochs") is not None:
        config['training']['num_train_epochs'] = args_dict["num_train_epochs"]
    if args_dict.get("learning_rate") is not None:
        config['training']['learning_rate'] = args_dict["learning_rate"]
    if args_dict.get("logging_steps") is not None:
        config['training']['logging_steps'] = args_dict["logging_steps"]
    if args_dict.get("per_device_train_batch_size") is not None:
        config['training']['per_device_train_batch_size'] = args_dict["per_device_train_batch_size"]
    if args_dict.get("gradient_accumulation_steps") is not None:
        config['training']['gradient_accumulation_steps'] = args_dict["gradient_accumulation_steps"]
    
    return config
